---
title: ME323 - Introdução aos Modelos Probabilísticos
subtitle: Parte 5
output:
  ioslides_presentation:
    widescreen: yes
logo: ../logo-imecc.png
---


# Principais Modelos Discretos


## Uniforme Discreta {.build}

**Exemplo:** Uma rifa tem 100 bilhetes numerados de 1 a 100. Tenho 5 bilhetes consecutivos numerados de 21 a 25, e meu colega tem outros 5 bilhetes, com os números 1, 11, 29, 68 e 93.  

Quem tem maior probabilidade de ser sorteado? 

>  + espalhar os números é a melhor forma de ganhar o sorteio? 
  
>  + assumindo honestidade da rifa, todos os números têm a mesma probabilidade de ocorrência, com $\frac{1}{100}$ para cada um.
  
>  + como eu e meu colega temos 5 bilhetes, temos a mesma probabilidade de ganhar a rifa: $\frac{5}{100}=\frac{1}{20}$.
   
>  + assim, a probabilidade de ganhar depende somente da quantidade de bilhetes que se tem na mão, independente da numeração.
  
## Uniforme Discreta {.build}

* A v.a. discreta $X$, assumindo valores $x_{1},x_{2},...,x_{k}$, segue uma distribuição uniforme discreta se, e somente se, cada valor possível tem a mesma probabilidade de ocorrer, isto é,

> $$P(X=x_i) = p(x_i) = \frac{1}{k}, \quad \forall \; 1\leq i \leq k$$

> **Exemplo:** lançamento de um dado honesto de 6 faces

$X$ | 1 | 2 | 3 | 4 | 5 | 6 
-- | :-: | :-: | :-: | :-: | :-: | :-: 
$p(x)$ | $\frac{1}{6}$ | $\frac{1}{6}$ | $\frac{1}{6}$ | $\frac{1}{6}$ | $\frac{1}{6}$ | $\frac{1}{6}$ 


## Uniforme Discreta 

A média e a variância são dadas por:

<font size=4>
$\displaystyle \mathbb E(X) = \sum_{i=1}^k x_iP(X=x_i)= \frac{1}{k}\sum_{i=1}^k x_i$

$\displaystyle Var(X) = \sum_{i=1}^k (x_i - \mathbb E(X))^2 P(X=x_i) = \frac{1}{k} \sum_{i=1}^k (x_i - \mathbb E(X))^2$

ou

$\displaystyle Var(X) = \mathbb E(X^2) - [\mathbb E(X)]^2 = \frac{1}{k} \sum_{j=1}^{k} x_{j}^{2} - \left (\frac{1}{k} \sum_{j=1}^{k}x_{j} \right)^{2} = \frac{1}{k} \left[\sum_{j=1}^{k}x_{j}^{2} - \frac{1}{k} \left (\sum_{j=1}^{k}x_{j} \right)^{2} \right]$
</font>


## Uniforme Discreta {.build}

> **Exemplo:** lançamento de um dado honesto de 6 faces

$X$ | 1 | 2 | 3 | 4 | 5 | 6 
-- | :-: | :-: | :-: | :-: | :-: | :-: 
$p(x)$ | $\frac{1}{6}$ | $\frac{1}{6}$ | $\frac{1}{6}$ | $\frac{1}{6}$ | $\frac{1}{6}$ | $\frac{1}{6}$ 

<br />

Calculando a esperança e a variância de $X$:

$\mathbb E(X) = \frac{1}{6} (1+2+3+4+5+6)= \frac{21}{6} = 3.5$

$Var(X) = \displaystyle \frac{1}{6} (1 + 4 + 9 + 16 + 25 + 36) - \left(\frac{21}{6} \right)^2 =  \frac{91}{6} - \frac{441}{36} = 2.92$


## Uniforme Discreta {.build}

Cálculo da função de distribuição acumulada (f.d.a.) de uma variável uniforme discreta:

$$F(x)= P(X\leq x)= \sum_{x_i \leq x} \frac{1}{k} = \frac{\#(x_i \leq x)}{k}$$

> **Exemplo:** voltando ao exemplo do lançamento de um dado honesto de 6 faces

* $F(2)= P(X\leq2)=P(X=1)+P(X=2)=\frac{2}{6}$

* $F(2.5)= P(X\leq2.5)= P(X=1)+P(X=2)=\frac{2}{6}$


## Uniforme Discreta - f.d.a

$X$ |$p(x)$ | $F(x)$ 
:--: | :-: | :-: 
1 | $\frac{1}{6}$ | $\frac{1}{6}$ 
2 | $\frac{1}{6}$ | $\frac{2}{6}$  
3 | $\frac{1}{6}$ | $\frac{3}{6}$ 
4 | $\frac{1}{6}$ | $\frac{4}{6}$ 
5 | $\frac{1}{6}$ | $\frac{5}{6}$  
6 | $\frac{1}{6}$ | $\frac{6}{6}$ 


## Uniforme Discreta - Gráficos

<center>
```{r, echo=FALSE, fig.height=5, fig.width=5}
library(RColorBrewer)
mycol <- brewer.pal(8,"Dark2")

p <- rep(1/6, 6)
par(mar=c(5, 4, 1, 2) + .1)
barplot(p, names.arg=1:6, col="blue", ylim=c(0, .25), ylab="p(x)", xlab="x", main="Distribuição de Probabilidade", cex.lab=1.5, cex.axis=1, cex.names=1, las=1)
box(bty="l", lwd=2)

source("../functions/plotcdf.R")
x <- 1:6
p <- rep(1/6, 6)
plotcdf(x, p)
```
</center>


## Bernoulli {.build}
Em muitas aplicações, cada observação de um experimento aleatório é **binária**: tem apenas dois resultados possíveis.  

> Por exemplo, uma pessoa pode:

>  + aceitar ou recusar uma oferta de cartão de crédito de seu banco.
   
>  + votar sim ou não em uma assembléia.


## Modelo Bernoulli {.build}

> * Considere um experimento aleatório com dois resultados possíveis: **sucesso** e **fracasso**.

> * Seja $p$ a probabilidade de sucesso.

> * **Exemplo:** lançar uma moeda e verificar se cai cara ou coroa. Consideramos como sucesso, a obtenção de cara. Se a moeda é honesta, $p=1/2$.

> * Esse tipo de experimento é conhecido como **Ensaio de Bernoulli**.


## Modelo Bernoulli {.build}

* **Exemplo:** lançar um dado, sendo "sucesso" o obtenção da face 6. Se o dado é honesto, a probabilidade de sucesso é $p=1/6$.

> * **Exemplo:** Uma pessoa é escolhida ao acaso entre os moradores de uma cidade, e é perguntado a ela se concorda com um projeto. 

> * As possíveis respostas são apenas "Sim"  ou "Não". 
$$\Omega=\{\mbox{Sim},\mbox{Não}\}$$

>
$$ X = \begin{cases}
1, & \mbox{se a pessoa respondeu sim } (sucesso) \\
0, & \mbox{caso contrário } (fracasso) \\
\end{cases}
$$

> $$P(X=1)=P(sucesso)=p  \;\; \rightarrow \;\; P(X=0)=P(fracasso)=1-p$$


## Modelo Bernoulli {.build}

Seja $X$ uma v.a. discreta assumindo apenas valores 0 e 1, onde $X=1$ corresponde a sucesso e seja $p$ a probabilidade de sucesso. 

A distribuição de probabilidade de $X$ é dada por:

$$ P(X=x) = \begin{cases}
p & \mbox{se } x=1 \\
1-p & \mbox{se } x=0 \\
\end{cases}
$$

Ou de forma equivalente, podemos escrever como:
$$P(X=x) = p^{x}(1-p)^{1-x},  \quad \mbox{para } x=0,1$$ 

**Notação:** $X \sim b(p)$


## Bernoulli - Esperança e Variância {.build}

Se $X$ é um v.a. Bernoulli, $X \sim b(p)$, então:

> $$\mathbb E(X) = p \quad \mbox{e} \quad Var(X)=p(1-p)$$

> **Demonstração:** 

> * **Esperança:** $\mathbb E(X)= 0\times(1-p) + 1\times p = p$

> * $\mathbb E(X^{2})= 0^{2}\times(1-p) + 1^{2}\times p = p$

> * **Variância:** 
$$\begin{aligned}
Var(X) &= \mathbb E(X^{2})-[\mathbb E(X)]^{2}  \\
& = p-p^{2} =p(1-p)
\end{aligned}
$$


## Bernoulli

A função de distribuição acumulada de uma v.a. Bernoulli é:
<font size=5>
$$ F(x) = \begin{cases}
0, & x < 0 \\
1-p, & 0 \leq x < 1 \\
1, & x \geq 1
\end{cases}
$$
</font>

```{r, echo=FALSE, fig.height=3.5, fig.width=4.5}
p <- c(.3, .7)

## Função de Massa
par(mar=c(c(5, 4, 2, 1) + 0.1))
barplot(p, names.arg=0:1, col="blue", ylim=c(0, 1.1), ylab="p(x)", xlab="x",  
        cex.lab=1.5, cex.axis=1.2, cex.names=1.5, yaxt="n", main="Distribuição de Probabilidade")
box(bty="l", lwd=2)
axis(2, at= c(0, p, 1), labels=c(0, "1-p", "p", 1), cex.axis=1.5, las=1)

## CDF
cdf <- c(0, cumsum(p))
par(mar=c(c(5, 4, 2, 1) + 0.1))
plot(0:1, cumsum(p), pch=20, ylim=c(0,1.1), xlim=c(-.5,1.5), xlab="x", ylab="F(x)",
     cex.lab=1.5, cex.axis=1.2, axes=FALSE, main="Função de Distribuição Acumulada")
box(bty="l", lwd=2)
axis(1, at=c(0,1), cex.axis=1.5)
axis(2, at=cdf, labels=c(0, "1-p", 1), cex.axis=1.5, las=1)
abline(h=1, lty=2, col=2, lwd=2)
segments(-1:1, cdf, x1=0:2-.01, col="blue", lwd=3)
points(0:1, cdf[-1], pch=19)
points(0:1, cdf[-length(cdf)], lwd=1)
arrows(x0=0:1, y0=c(0, p[1])+.03, y1=c(p[1],1)-.03, length=.1, code=3, col="red")
text(c(0.2, 1.1), c(0.15, 0.7), labels=c("1-p", "p"), cex=1.2)
```


## Bernoulli {.build}

**Exemplo:** lançamos um dado e consideramos sucesso a obtenção da face 5

> Supondo que o dado seja honesto, a probabilidade de sucesso é $p = 1/6$. Então:

> $$ \begin{aligned} P(X=x) &= \left(\frac{1}{6} \right)^x \left(\frac{5}{6} \right)^{1-x} \qquad \mbox{para } x=0,1 \\
&= \begin{cases}
5/6 & \mbox{se } x=0 \\
1/6 & \mbox{se } x=1 \\
\end{cases}
\end{aligned}
$$

> * Encontre a esperança e variância de X.

>  $\qquad \mathbb E(X)=\frac{1}{6} \qquad \qquad \mathbb E(X^2)=\frac{1}{6}$

>  $\qquad Var(X)=\frac{1}{6}-\frac{1}{36}=\frac{6-1}{36}=\frac{5}{36}$ 


## Modelo Binomial {.build}

Ao obtermos uma amostra do experimento/fenômeno aleatório com observações binárias, podemos resumir os resultados usando o número ou a proporção de observações com o resultado de interesse.

> Sob certas condições, a v.a. $X$ que conta o número de vezes que um resultado específico ocorreu, dentre dois possíveis, tem uma distribuição de probabilidade chamada **Binomial**. 


* Considere um experimento aleatório com espaço amostral $\Omega$ e o evento $A$.

> * Vamos dizer que ocorreu sucesso se o evento $A$ aconteceu. Se $A$ não aconteceu ocorreu fracasso. 

> * Repetimos o experimento $n$ vezes, de forma independente.

> * Seja $X$ o número de sucessos nos $n$ experimentos.


## Exemplo: vacinas {.build}

Sabe-se que a eficiência de uma vacina é de $80\%$.  

Um grupo de 3 indivíduos é sorteado, dentre a população vacinada, e cada um é submetido a testes para averiguar se está imunizado. 

Nesse caso, consideramos como sucesso a imunização.

>
$$ X_i = \begin{cases}
1, & \mbox{indivíduo i está imunizado} \\
0, & \mbox{caso contrário}
\end{cases}
$$

>  Pelo enunciado, sabe-se que $P(X_{i}=1)=p=0.8$.


## Exemplo: vacinas {.build}

* Os indivíduos 1, 2 e 3 são independentes.  

> * As v.a.'s $X_1$, $X_2$ e $X_3$ são Bernoulli. 

> * Se o interesse está em estudar $X=$ número de indivíduos imunizados no grupo, $X$ poderá assumir valores $\{0,1,2,3\}$. 

> * Note que $X=X_{1}+X_{2}+X_{3}$.


## Exemplo: vacinas {.smaller}

evento | P(evento) | X 
:------: | --------- | :-:
$X_{1}=0$, $X_{2}=0$, $X_{3}=0$ | $(0.2)^{3}$ | 0 
$X_{1}=1$, $X_{2}=0$, $X_{3}=0$ | $0.8\times(0.2)^{2}$ | 1 
$X_{1}=0$, $X_{2}=1$, $X_{3}=0$ | $0.8\times(0.2)^{2}$ | 1 
$X_{1}=0$, $X_{2}=0$, $X_{3}=1$ | $0.8\times(0.2)^{2}$ | 1 
$X_{1}=1$, $X_{2}=1$, $X_{3}=0$ | $(0.8)^{2}\times0.2$ | 2 
$X_{1}=1$, $X_{2}=0$, $X_{3}=1$ | $(0.8)^{2}\times0.2$ | 2 
$X_{1}=0$, $X_{2}=1$, $X_{3}=1$ | $(0.8)^{2}\times0.2$ | 2 
$X_{1}=1$, $X_{2}=1$, $X_{3}=1$ | $(0.8)^{3}$ | 3 


## Modelo Binomial {.build}

Assim, as probabilidades de cada valor possível de $X$ são:

>
$X$ | 0 | 1 | 2 | 3 
--- | :-: | :-: | :-: | :-:
$P(X=x)$ | $(0.2)^{3}$ | $3\times0.8\times(0.2)^{2}$ | $3\times(0.8)^{2}\times0.2$ | $(0.8)^{3}$ 

<br />
O comportamento de $X$ é completamente determinado pela função: 

> $$P(X=x)={3\choose x}(0.8)^{x}(0.2)^{3-x}, \qquad x=0,1,2,3.$$


## Binomial {.build}
**Modelo Geral:** Considere a repetição de $n$ ensaios $X_i$ Bernoulli independentes e todos com a mesma probabilidade de sucesso $p$.  

> A variável aleatória $X=X_{1}+...+X_{n}$ representa o total de sucessos e corresponde ao modelo Binomial com parâmetros $n$ e $p$, ou seja, $X\sim Bin(n,p)$.

> A probabilidade de se observar $x$ é dada pela expressão geral: 
> $$P(X=x)={n\choose x}p^{x}(1-p)^{n-x}, \qquad x=0,1,...,n$$

> A esperança e variância de uma v.a. Binomial são dadas por:
> $$\mathbb E(X)=np \qquad \mbox{e} \qquad Var(X)=np(1-p)$$


## Modelo Binomial
Distribuição de probabilidade de uma $Bin(3, p)$, com $p=0.2, 0.5$ e $0.8$.

<center>
```{r, echo=FALSE, fig.height=4, fig.width=9}
binomial <- function(n, ...){
  ps <- c(.2,.5,.8)
  x <- 0:n
  par(mfrow=c(1,3))
  for(i in 1:3){
    px <- dbinom(x, size=n, prob=ps[i])
    barplot(px, names.arg=0:n, col="blue", main=paste("Bin(", n, ",",  ps[i], ")", sep=""), 
            cex.lab=2, cex.axis=1.5, cex.names=1.5, las=1, ...)
    box(bty="l", lwd=2)
  }
}
binomial(3, ylim=c(0, .5))
## binomial(10)
## binomial(30)
```
</center>


## Modelo Binomial
Distribuição de probabilidade de uma $Bin(10, p)$, com $p=0.2, 0.5$ e $0.8$.
<center>
```{r, echo=FALSE, fig.height=4.5, fig.width=9}
binomial(10, ylim=c(0, .3))
```
</center>


## Exemplo: Vacina {.build}

* No exemplo da vacina, temos então que o número de indíviduos imunizados segue uma distribuição Binomial com $n=3$ e $p=0.8$

> $$X\sim Bin(3,0.8)$$

> * Qual a probabilidade de que dentre os 3 indíviduos, nenhum tenha sido imunizado?
> $$ P(X=0) = {3 \choose 0} (0.8)^0 (0.2)^3 = `r dbinom(0, 3, .8)`$$

> * Encontre a esperança e variância:

> $$\mathbb E(X)=3\times 0.8 = `r 3 * 0.8` \qquad \mbox{e} \qquad Var(X)=3\times0.8\times0.2 = `r 3*.8*.2`$$


## Exemplo: Inspeção {.build}

Um inspetor de qualidade extrai uma amostra aleatória de 10 tubos armazenados num depósito onde, de acordo com os padrões de produção, se espera um total de $20\%$ de tubos defeituosos. 

> Qual é a probabilidade de que não mais do que $2$ tubos extraídos sejam defeituosos? 

> Se $X$ denotar a variável "número de tubos defeituosos em 10 extrações independentes e aleatórias", qual o seu valor esperado? Qual a variância?


## Exemplo: Inspeção {.build}

Note que a variável aleatória $X$ = número de tubos defeituosos em 10 extrações tem distribuição binomial, com parâmetros $n=10$ e $p=0.2$.

> Então, "não mais do que dois tubos defeituosos" é o evento $\{X\leq2\}$. 

> Sabemos que, para $X \sim Bin(10, 0.2)$ 
> $$P(X=x) = { 10\choose x } (0.2)^x (0.8)^{10-x}, \qquad x=0,1, \ldots, 10$$ 

> e que 
$$\begin{aligned}
P(X\leq2) &=  P(X=0) + P(X=1) + P(X=2) \\
&=(0.8)^{10} + 10 (0.2)(0.8)^{9} + 45 (0.2)^2 (0.8)^8 = 0.678
\end{aligned}
$$


## Exemplo: Inspeção {.build}

Se $X \sim Bin(n,p)$, então $\mathbb E(X) = np$ e $Var(X) = np(1-p)$ 

> Então:

> $\mathbb E(X) = 10 (0.2) = 2$

> $Var(X) = 10 (0.2)(0.8) = 1.6$

> Quando se encontram quatro ou mais tubos defeituosos, o processo de produção é interrompido para revisão. Qual é a probabilidade que isto aconteça?

>
$$ \begin{aligned}
P(X\geq 4) &=  1 - P(X < 4) \\
&= 1 - P(X \leq 3) \\
&= 1-0.879 = 0.121
\end{aligned} $$


## Exemplo: Comprador A ou B? {.build}

**Exemplo:** Um industrial fabrica peças, das quais 1/5 são defeituosas.  Dois compradores **A** e **B** classificaram um grande lote de peças adquiridas em categorias $I$ e $II$, pagando $\$ 1.20$ e $\$ 0.80$ por peça, respectivamente, do seguinte modo: 

* **Comprador A**: retira uma amostra de cinco peças; se encontrar mais que uma defeituosa, classifica como $II$. 

> * **Comprador B**: retira uma amostra de dez peças; se encontrar mais que duas defeituosas, classifica como $II$. 

> Em média, qual comprador oferece mais lucro?

> *Fonte: Morettin \& Bussab*, Estatística Básica $5^a$ edição, pág 159.


## Exemplo: Comprador A ou B? {.build}

Sabemos que 1/5 das peças são defeituosas.  

Podemos nos concentrar na probabilidade dos vendedores julgarem um lote como tipo $I$ ou $II$. 

Seja $X$ o número de peças defeituosas em $n$ testes. 

O experimento do **comprador A** tem distribuição $X_A \sim Bin(5, 1/5)$  enquanto o experimento do **comprador B** tem distribuição $X_B \sim Bin(10, 1/5)$. 

Para o **comprador A**, temos que: 
$$ \begin{aligned}
P(X_A > 1 ) &= 1 - P(X_A = 0 ) - P(X_A = 1 ) \\
&= 1 - {5 \choose 0} \left (1-\frac{1}{5} \right)^5- {5 \choose 1} \left( \frac{1}{5} \right) \left (1-\frac{1}{5} \right)^4 = 0.263
\end{aligned}$$ 


## Exemplo: Comprador A ou B? {.build}

De modo similar, para o **comprador B** temos:
$$\begin{aligned}
P(X_B > 2 ) &= 1 - {10 \choose 0} \left (1-\frac{1}{5} \right )^{10} - {10 \choose 1} \frac{1}{5} \left (1-\frac{1}{5} \right )^9 \\
&- {10 \choose 2} \left( \frac{1}{5} \right)^2 \left (1-\frac{1}{5} \right )^8 = 0.322
\end{aligned}$$ 

Como o segundo comprador irá classificar o lote como $II$ com maior probabilidade que o primeiro, ele é o que oferece menor lucro para o fornecedor. 

Mas podemos verificar o lucro esperado do vendedor. 


## Exemplo: Comprador A ou B? {.build}

Preço por peça na categoria $I$: \$1.20. Preço por peça na categoria $II$: \$0.80.

Se o industrial decidir vender o lote para o **comprador A**, temos que  

$$\mathbb E(\mbox{lucro A}) = 1.20 \times 0.737 + 0.80 \times 0.263 \approx 1.09$$ 

Ou seja, ele irá lucrar em média $\$ 1.09$ por peça. 
 
Já se ele vender para o **comprador B**, temos que 
 
$$\mathbb E(\mbox{lucro B}) =  1.20 \times 0.678 + 0.80 \times 0.322 \approx 1.07$$ 
 
que é um lucro dois centavos inferior. 
 
Portanto, é mais interessante ao industrial que o comprador A examine mais peças.






# Distribuição Geométrica

## Distribuição Geométrica {.build}

Consideremos novamente um experimento aleatório com espaço de resultados $\Omega$ e o evento $A$. 

Vamos dizer que ocorreu sucesso se o evento $A$ aconteceu e $p=P(\mbox{sucesso})$.  

Repetimos o experimento até o primeiro sucesso. 

Seja $X$ o número de repetições até o primeiro sucesso.

**Exemplo:** lançar uma moeda repetidas vezes até a primeira cara e $p=P(cara)$.

Os valores possíveis de $X$ são $\{1,2,3,...\}$. 

$$ \begin{aligned} 
P(X=1) &=p  && (\mbox{sucesso logo na primeira tentativa})\\
P(X=2) &= (1-p)p  && (\mbox{1 fracasso seguido de 1 sucesso})\\
P(X=k) &= (1-p)^{k-1} p && (\mbox{$k-1$ fracassos sucessivos e 1 sucesso})
\end{aligned}
$$



## Distribuição Geométrica {.build}

**Modelo Geral:** Suponha uma sequência de ensaios de Bernoulli independentes com probabilidade de sucesso $p$. 

> Seja $X$ a v.a. que representa o número de ensaios de Bernoulli até a ocorrência do primeiro sucesso. Então dizemos que $X$ segue uma distribuição **Geométrica** com parâmetro $p$, ou seja, $X\sim G(p)$.

> A probabilidade de se observar $x$ é dada por: 
$$P(X=x)=(1-p)^{x-1}p, \qquad x=1,2,\ldots$$

> A esperança e variância de uma v.a. Geométrica são dadas por:
$$\mathbb E(X)= \frac{1}{p} \qquad \mbox{e} \qquad Var(X)=\frac{1-p}{p^2}$$


## Distribuição Geométrica
Distribuição de probabilidade de uma $G(p)$, com $p=0.3, 0.5$ e $0.7$.

<center>
```{r, echo=FALSE, fig.height=4, fig.width=9}
geometrica <- function(n, ps=c(0.2, 0.5, 0.8), ...){
  x <- 0:(n-1)
  par(mfrow=c(1,3))
  for(i in 1:3){
    px <- dgeom(x, prob=ps[i])
    barplot(px, names.arg=x+1, col="blue", main=paste("G(", ps[i], ")", sep=""), 
            cex.lab=1.5, cex.axis=1.3, cex.names=1.3, cex.main=2, las=1, xlab="x", ylab="P(X=x)", ...)
    box(bty="l", lwd=2)
  }
}
geometrica(10, ps=c(0.3, 0.5, 0.7), ylim=c(0, .7))
```
</center>





## Distribuição Geométrica {.build}

A função de distribuição acumulada de uma v.a. $G(p)$ é dada por:
$$F(x)=P(X\leq x)=1-(1-p)^x$$

A distribuição geométrica tem uma propriedade que serve para caracterizá-la no conjunto das distribuições discretas: a propriedade de perda de memória!

**Propriedade de Perda de Memória**

$$P(X > x+m \mid X > m)=P(X > x)$$

**Interpretação:** O fato de já termos observado $m$ fracassos sucessivos não muda a probabilidade do número de ensaios até o primeiro sucesso ocorrer.


## Propriedade de perda de memória

$$P(X > x+m \mid X > m)=P(X > x)$$

**Demonstração:** 

Lembre-se que:
$$F(x)=P(X\leq x)=1-(1-p)^x \quad \Longrightarrow \quad P(X > x)= (1-p)^{x}$$ 

Então,
$$
\begin{aligned}
P(X > x+m \mid X > m) &= \frac{P(X > x+m, X> m)}{P(X> m)} \\
& = \frac{P(X > x+m)}{P(X> m)} = \frac{(1-p)^{x+m}}{(1-p)^m} \\
&=(1-p)^{x} = P(X> x) \\
\end{aligned}
$$


## Exemplo: Sinal de trânsito {.build}

A probabilidade de se encontrar aberto o sinal de trânsito numa esquina é $0.2$.  

Qual a probabilidade de que seja necessário passar pelo local $5$ vezes para encontrar o sinal aberto pela primeira vez?

$X=$ número de vezes necessárias para encontrar o sinal aberto. 

$p=P(\mbox{sinal aberto}) = 0.2$ 

$$\begin{aligned}
P(X=5) &= (1-p)^{4} p \\
&= 0.8^4\times 0.2 \\
&= 0.0819
\end{aligned} $$


## Exemplo: lançamento de um dado {.build}

Qual a probabilidade de que um dado deva ser lançado 15 vezes para que ocorra a face 6 pela primeira vez?

$X=$ número de vezes necessárias para ocorrer o resultado 6. 

$p=P(\mbox{face 6})=1/6$ 

$$ \begin{aligned}
P(X=15) &= (1-p)^{15-1} p \\
&= \left(\frac{5}{6}\right)^{14}\frac{1}{6} \\
&=0.01298
\end{aligned} $$


## Exemplo: Roleta Russa {.build}

Em sua autobiografia _A Sort of Life_, o autor inglês Graham Greene descreveu um período de grave depressão em que ele jogava roleta russa.  Esse "jogo" consiste em colocar uma bala em uma das seis câmaras de um revólver, girar o tambor e disparar a arma contra a própria cabeça. 

> * Greene jogou seis partidas deste jogo, e teve a sorte da arma nunca ter disparado.  Qual a probabilidade desse resultado? 

> * Suponha que ele continue jogando roleta russa até a arma finalmente disparar.  Qual é a probabilidade de Greene morrer na $k$-ésima jogada? 

*Fonte: A. Agresti*, Categorical Data Analysis.

## Exemplo: Roleta Russa {.build}

> * Greene jogou seis partidas deste jogo, e teve a sorte da arma nunca ter disparado.  Qual a probabilidade desse resultado? 

> Ao girar o tambor, a arma disparar ou não é um ensaio de Bernoulli com probabilidade $1/6$ de disparar. Como cada uma das jogadas é independente, a probabilidade da arma não ter disparado em nenhuma das seis vezes é

$$\left( \frac{5}{6} \right)^6 = 0.33489 $$

Seja $X$ o número de disparos na roleta russa em 6 partidas e $p=1/6$ a probabilidade de disparo. Então, $X \sim Bin(6, 1/6).$

$$P(X=0)=\binom{6}{0}p^0(1-p)^{6-0} = \left( \frac{5}{6} \right)^6 = 0.33489$$


## Exemplo: Roleta Russa {.build}

> * Qual é a probabilidade de Greene morrer na $k$-ésima jogada? 

> Ao efetuar a 1ª jogada, o autor pode morrer com probabilidade $1/6$, ou continuar jogando. Se ele sobreviver à primeira, pode jogar pela 2ª vez, e morrer com probabilidade $5/6 \times 1/6$, ou continuar jogando. Repetindo esse raciocínio, a probabilidade de morte na $k$-ésima jogada é  
$$\left( \frac{5}{6} \right)^{k-1} \frac{1}{6}$$ 

Ou podemos definir $X$ como o número de tentativas até o 1º disparo e $p=1/6$ a probabilidade de um disparo ocorrer. Então, $X \sim G(1/6)$ e:
$$P(X=k) = \left( \frac{5}{6} \right)^{k-1} \frac{1}{6}$$ 


## Exemplo: Banco de Sangue {.build}

Um banco de sangue necessita sangue do tipo O negativo. Suponha que a probabilidade de uma pessoa ter este tipo de sangue seja $0.10$.  Doadores permanentes chegam ao hemocentro para fazer sua doação rotineira. Calcule a probabilidade de que o primeiro doador com sangue do tipo O negativo seja:

> * o primeiro a chegar; 
> * o segundo; 
> * o sétimo. 
> * Quantos doadores esperamos passar pelo hospital até encontrarmos um com sangue O negativo? 

> **Fonte:** Prof. Mario Gneri, Notas de Aula.


## Exemplo: Banco de Sangue {.build}

Seja $X$ o número de doadores que chegam no hemocentro até a chegada do primeiro doador com sangue O negativo.

Novamente temos um experimento com distribuição geométrica. Usando a fórmula para a função de probabilidade, send $X \sim G(0.1)$:

$$P(X=x) = 0.9^{x-1} 0.1, \qquad x=1,2,\ldots $$ 

Temos que 

> * $P(X=1) =  0.1$
> * $P(X=2) =  0.9 \times 0.1 = 0.09$ 
> * $P(X=7) =  0.9^6 \times 0.1 = 0.053$  
> * $\mathbb E(X) = 1/0.1=10.$ Neste caso, esperamos que dez doadores passem pelo hospital, em média, para encontrarmos o primeiro com sangue O negativo.



## Leituras

* Devore: capítulo 3.
* [Ross](http://www.sciencedirect.com/science/book/9780123743886): capítulo 5
* Magalhães: capítulo 3

##

Slides produzidos pelos professores:

* Samara Kiihl

* Tatiana Benaglia

* Benilton Carvalho